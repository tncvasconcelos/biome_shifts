---
title: 'SX: Regression Distribution Results'
output:
  pdf_document: default
  html_notebook: default
---

## Label switching and the impact on regression results

One of the quirks of hidden Markov models (HMMs) is that the labeling of rate classes is arbitrary. Not in the sense that labels are meaningless, but rate that it makes no difference whether something is labeled as A or B (a rate by any other name would smell as sweet). 

This presents a problem for optimization since this means that there are two parameterizations with exactly the same likelihood (swapping rate class A and rate class B). Within packages like corHMM or hisse we have tried to circumvent this issue by generally forcing one rate class to be faster than another. This is done internally to help with the likelihood search but should also provide more consistency in the output results. Nonetheless, this issue could still impact our analysis of parameter estimates.

The impact on our regression estimates would be due to the fact that whether we are subtracting B from A is not consistent across our datasets. This means that you could comfortably flip, for any given clade, whether you are subtracting A from B or B from A. In this vignette, I will run a randomization test to determine whether the default results are trustworthy 

Let's review our initial results. 


```{r, echo=FALSE, results = 'hide', message=FALSE, warning=FALSE}
# rm(list=ls())
setwd("~/biome_shifts/")
source("00_utility_functions.R")

library(ape)
library(hisse)
library(parallel)
library(phytools)
library(phylolm)
library(phytools)
library(RColorBrewer)

##############################
### load data
##############################

plot_data <- read.csv("all_par_table.csv")
plot_data <- plot_data[plot_data$Group != "Quercus",]
plot_data <- plot_data[plot_data$Group != "Araceae",]

##############################
### correcting tip labels
##############################

phy_bb <- read.tree("backbone_tree.tre")
dat_names <- unlist(lapply(strsplit(paste(plot_data$Group), " "), function(x) x[[1]]))
phy_bb$tip.label <- unlist(lapply(strsplit(phy_bb$tip.label, "-"), function(x) x[[1]]))
phy_bb$tip.label <- unlist(lapply(strsplit(phy_bb$tip.label, "_"), function(x) x[[1]]))
to_drop <- phy_bb$tip.label[!phy_bb$tip.label %in% dat_names]
phy_bb <- drop.tip(phy_bb, to_drop)
# plotTree(phy_bb, fsize=0.75, ftype="i")
clade_names_sorted <- tip_names <- phy_bb$tip.label


##############################
### regression of mean rateclass rate against turnover
##############################
rate_class_a_rate <- cbind(plot_data$f00_t01_a,
                           plot_data$f01_t00_a,
                           plot_data$f01_t11_a,
                           plot_data$f11_t01_a)
rate_class_b_rate <- cbind(plot_data$f00_t01_b,
                           plot_data$f01_t00_b,
                           plot_data$f01_t11_b,
                           plot_data$f11_t01_b)
rate_class_a_turn <- cbind(plot_data$tn_00_a,
                           plot_data$tn_01_a,
                           plot_data$tn_11_a)
rate_class_b_turn <- cbind(plot_data$tn_00_b,
                           plot_data$tn_01_b,
                           plot_data$tn_11_b)
obs_00_turn <- cbind(plot_data$tn_00_a,
                     plot_data$tn_00_b)
obs_01_turn <- cbind(plot_data$tn_01_a,
                     plot_data$tn_01_b)
obs_11_turn <- cbind(plot_data$tn_11_a,
                     plot_data$tn_11_b)


# scatter plot of diff
lm_dat <- data.frame(row.names = gsub(" .*", "", plot_data[,1]),
                     d_trans = log(rowMeans(rate_class_b_rate)) - 
                       log(rowMeans(rate_class_a_rate)),
                     d_turns = log(rowMeans(rate_class_b_turn)) - 
                       log(rowMeans(rate_class_a_turn)))

```

```{r, echo=FALSE}
fit = phylolm(d_turns ~ d_trans, data=lm_dat, phy=phy_bb, boot = 1000)
print(summary(fit))
plot.new()
# Set up the full plot area
par(fig=c(0, 1, 0, 1), mar=c(.1,.1,.1,.1), new=TRUE)
plot(type="n", x = lm_dat$d_trans, y = lm_dat$d_turns, bty='n',
     xaxt='n', yaxt='n', xlab = "", ylab = "", 
     xlim=range(lm_dat$d_trans))
grid()
# axes lines
lines(x = round(c(min(lm_dat$d_trans), max(lm_dat$d_trans))), y = c(0, 0), lwd=2)
lines(x = c(0, 0), y = round(c(min(lm_dat$d_turns), max(lm_dat$d_turns))), lwd=2)

# custom x
ticks_x <- round(seq(from = min(lm_dat$d_trans), to = max(lm_dat$d_trans+1), by = 2))
ticks_x <- ticks_x[-c(3,4)]
segments(x0 = ticks_x, y0 = -0.05, x1 = ticks_x, y1 = 0.05) 
text(x = ticks_x, y = -0.2, labels = round(ticks_x, 2), srt = 0, adj = 0.5)
text(x = max(lm_dat$d_trans), y = -0.6, labels = expression(Delta * log(transition~rates)), srt = 0, adj = 1, cex = 1.25)

# custom y
ticks_y <- round(seq(from = min(lm_dat$d_turns), to = max(lm_dat$d_turns), by = 2))
ticks_y <- ticks_y[-c(3,4,5)]
segments(x0 = -0.05, y0 = ticks_y, x1 = 0.05, y1 = ticks_y)
text(x = 0.2, y = ticks_y, labels = round(ticks_y, 2), srt = 0, adj = 0, xpd = TRUE)
text(x = -0.4, y = max(lm_dat$d_turns), labels = expression(Delta * log(turnover~rates)), srt = 90, adj = 1, cex = 1.25)

# prepare CI data 
X <- cbind(1, 
           seq(from=min(lm_dat$d_trans)*2, to=max(lm_dat$d_trans)*2, length.out=100))
Y_hat <- X %*% fit$coefficients  # Predicted values
Var_Y_hat <- diag(X %*% fit$vcov %*% t(X))
t_value <- qt(0.975, df = fit$n - length(fit$coefficients))
CI_lower <- Y_hat - t_value * sqrt(Var_Y_hat + fit$sigma2)
CI_upper <- Y_hat + t_value * sqrt(Var_Y_hat + fit$sigma2)
CI_results <- data.frame(Y_hat = as.vector(Y_hat), 
                         CI_lower = as.vector(CI_lower), 
                         CI_upper = as.vector(CI_upper))
X_plot <- X[,2]
sorted_indices <- order(X_plot) 
X_plot <- X_plot[sorted_indices]
sorted_Y_hat <- CI_results$Y_hat[sorted_indices]
sorted_CI_lower <- CI_results$CI_lower[sorted_indices]
sorted_CI_upper <- CI_results$CI_upper[sorted_indices]
mat_data <- cbind(sorted_Y_hat, sorted_CI_lower, sorted_CI_upper)

# plotting
matlines(X_plot, mat_data, lty = c(1, 2, 2), col = c("#737373", "#6BAED6", "#6BAED6"), lwd = c(2,1,1))
polygon(c(X_plot, rev(X_plot)), c(sorted_CI_lower, rev(sorted_CI_upper)), col = rgb(0.6784314, 0.8470588, 0.9019608, .25), border = NA)

# add points
palette <- brewer.pal(12, "Set3")
colors <- rep(palette, length.out = 51)
# Define the corners of the rectangle
x1 <- 2; y1 <- -6   # Bottom left
x2 <- 6; y2 <- -6  # Bottom right
x3 <- 2; y3 <- -2  # Top left
x4 <- 6; y4 <- -2 # Top right
n_col <- 5  # Columns
n_row <- 10 # Rows
x_seq <- seq(x1, x2, length.out = n_col)
y_seq <- seq(y1, y3, length.out = n_row)
grid_points <- expand.grid(x = x_seq, y = y_seq)
for(i in seq_along(clade_names_sorted)){
  points(x = lm_dat$d_trans[i], y = lm_dat$d_turns[i],
         pch = 21, bg = colors[i], cex=3)
  text(x = lm_dat$d_trans[i], y = lm_dat$d_turns[i],
       label=i, cex = .85)
  # points(x = grid_points$x[i], y = grid_points$y[i],
  #        pch = 21, bg = colors[i], cex=1)
  # text(x = grid_points$x[i], y = grid_points$y[i],
  #      label=clade_names_sorted[i], cex = .25)
}
```

The results of our initial regression are shown above. We find a significant non-zero slope. However, as discussed above, for each clade, rate class a and rate class b are interchangeable. If we shuffle the rate classes around randomly, we will get a different result. 

Let's look at the raw data as a starting point. The data shown below are the transition rates for all possible transitions in our model with the order as follows: 
$q_{00:01(A)},q_{01:00(A)},q_{01:11(A)},q_{11:01(A)},q_{00:01(B)},q_{01:00(B)},q_{01:11(B)},q_{11:01(B)}$


```{r}
demo_df <- data.frame( a = head(rate_class_a_rate),
                       b = head(rate_class_b_rate))
print(demo_df)
```

## Random shuffling for a null distribution

To begin with let's try and examine a null expectation. In this case, if we were to randomly shuffle all transition rates and turnover rates (mixing rate class A and B) we would not expect to find an association. By shuffling rate classes any estimated differences between rate class A and rate class B should be removed.


```{r}
shuffled_rates <- cbind(rate_class_a_rate, rate_class_b_rate)
shuffled_rates <- apply(shuffled_rates, 1, 
                        function(x) x[sample(1:8,8)])
shuffled_rate_class_a_rate <- t(shuffled_rates)[,1:4]
shuffled_rate_class_b_rate <- t(shuffled_rates)[,5:8]


shuffled_turns <- cbind(rate_class_a_turn, rate_class_b_turn)
shuffled_turns <- apply(shuffled_turns, 1, 
                        function(x) x[sample(1:6,6)])
shuffled_rate_class_a_turn <- t(shuffled_turns)[,1:3]
shuffled_rate_class_b_turn <- t(shuffled_turns)[,4:6]

d_turns <- d_trans <- c()
for(i in 1:49){
  d_trans <- (c(d_trans, log(rowMeans(shuffled_rate_class_b_rate)[i]) - 
                  log(rowMeans(shuffled_rate_class_a_rate)[i])))
  d_turns <- (c(d_turns, log(rowMeans(shuffled_rate_class_b_turn)[i]) - 
                  log(rowMeans(shuffled_rate_class_a_turn)[i])))
}

lm_dat_null <- data.frame(row.names = gsub(" .*", "", plot_data[,1]),
                     d_trans = d_trans,
                     d_turns = d_turns)
fit_null = phylolm(d_turns ~ d_trans, data=lm_dat_null, phy=phy_bb, boot = 0)
plot(lm_dat_null)
```

```{r}
summary(fit_null)
```

This suggests this shuffling procedure did succesfully remove the signal as the slope is not significantly different from 0. the Let's iterate this shuffling procedure 1000 times and create a null distribution of slope estimates.

```{r, cache=TRUE}
all_fits_shuffled <- list()
for(j in 1:1000){
  shuffled_rates <- cbind(rate_class_a_rate, rate_class_b_rate)
  shuffled_rates <- apply(shuffled_rates, 1, 
                          function(x) x[sample(1:8,8)])
  shuffled_rate_class_a_rate <- t(shuffled_rates)[,1:4]
  shuffled_rate_class_b_rate <- t(shuffled_rates)[,5:8]
  
  
  shuffled_turns <- cbind(rate_class_a_turn, rate_class_b_turn)
  shuffled_turns <- apply(shuffled_turns, 1, 
                          function(x) x[sample(1:6,6)])
  shuffled_rate_class_a_turn <- t(shuffled_turns)[,1:3]
  shuffled_rate_class_b_turn <- t(shuffled_turns)[,4:6]
  
  d_turns <- d_trans <- c()
    for(i in 1:49){
      d_trans <- (c(d_trans, log(rowMeans(shuffled_rate_class_b_rate)[i]) - 
                      log(rowMeans(shuffled_rate_class_a_rate)[i])))
      d_turns <- (c(d_turns, log(rowMeans(shuffled_rate_class_b_turn)[i]) - 
                      log(rowMeans(shuffled_rate_class_a_turn)[i])))
    }
  lm_dat_shuffle <- data.frame(row.names = gsub(" .*", "", plot_data[,1]),
                       d_trans = d_trans,
                       d_turns = d_turns)
  fit_shuffle = phylolm(d_turns ~ d_trans, data=lm_dat_shuffle, phy=phy_bb, boot = 0)
  all_fits_shuffled[[j]] <- fit_shuffle
}
```

Distribution of the p-values and slope estimates.

```{r}
shuffled_est <- lapply(all_fits_shuffled, 
                       function(x) summary(x)$coefficients[2,c(1,4)])
shuffled_est_df <- do.call(rbind,shuffled_est)

par(mfrow=c(1,2))
hist(shuffled_est_df[,1], main = "slope estimates", xlab = "slope value")
hist(shuffled_est_df[,2], main = "p-values", xlab = "pvalue")
```

## Random shuffling for robustness against label switching

With a null expectation established let's shuffle rate classes in a way that should preserve the signal by randomly shuffling labels. The shuffling will randomly select between 1 and 49 clades and then randomly choose which of those clades will swap the labels on the rate class A and B. 

```{r}
no_to_swap <- sample(1:49, 1)
to_swap <- sample(1:49, no_to_swap, FALSE)
d_turns <- d_trans <- c()
for(i in 1:49){
  if(i %in% to_swap){
    d_trans <- (c(d_trans, log(rowMeans(rate_class_a_rate)[i]) - 
                    log(rowMeans(rate_class_b_rate)[i])))
    d_turns <- (c(d_turns, log(rowMeans(rate_class_a_turn)[i]) - 
                    log(rowMeans(rate_class_b_turn)[i])))
  }else{
    d_trans <- (c(d_trans, log(rowMeans(rate_class_b_rate)[i]) - 
                    log(rowMeans(rate_class_a_rate)[i])))
    d_turns <- (c(d_turns, log(rowMeans(rate_class_b_turn)[i]) - 
                    log(rowMeans(rate_class_a_turn)[i])))
  }
}

lm_dat_shuffle <- data.frame(row.names = gsub(" .*", "", plot_data[,1]),
                     d_trans = d_trans,
                     d_turns = d_turns)
fit_shuffle = phylolm(d_turns ~ d_trans, data=lm_dat_shuffle, phy=phy_bb, boot = 0)
plot(lm_dat_shuffle)
```

If we look at the data we can see that the data has been shuffled. 

```{r}
summary(fit_shuffle)
```
However, we are still able to recover a significantly positive slope. Lets repeat this procedure 1000 times.

```{r, cache=TRUE}
all_fits <- list()
for(j in 1:1000){
  no_to_swap <- sample(1:49, 1)
  to_swap <- sample(1:49, no_to_swap, FALSE)
  d_turns <- d_trans <- c()
  for(i in 1:49){
    if(i %in% to_swap){
      d_trans <- (c(d_trans, log(rowMeans(rate_class_a_rate)[i]) - 
                      log(rowMeans(rate_class_b_rate)[i])))
      d_turns <- (c(d_turns, log(rowMeans(rate_class_a_turn)[i]) - 
                      log(rowMeans(rate_class_b_turn)[i])))
    }else{
      d_trans <- (c(d_trans, log(rowMeans(rate_class_b_rate)[i]) - 
                      log(rowMeans(rate_class_a_rate)[i])))
      d_turns <- (c(d_turns, log(rowMeans(rate_class_b_turn)[i]) - 
                      log(rowMeans(rate_class_a_turn)[i])))
    }
  }
  
  lm_dat_tmp <- data.frame(row.names = gsub(" .*", "", plot_data[,1]),
                       d_trans = d_trans,
                       d_turns = d_turns)
  fit_tmp = phylolm(d_turns ~ d_trans, data=lm_dat_tmp, phy=phy_bb, boot = 0)
  all_fits[[j]] <- fit_tmp
}
```

Let's plot the resulting distributions. 

```{r}
est <- lapply(all_fits, 
              function(x) summary(x)$coefficients[2,c(1,4)])
est_df <- do.call(rbind,est)

par(mfrow=c(1,2))
hist(est_df[,1], main = "slope estimates", xlab = "slope value")
hist(est_df[,2], main = "p-values", xlab = "pvalue")
```

## Comparing distributions


```{r}
# Calculate the combined range for the slope estimates
min_slope = min(c(shuffled_est_df[,1], est_df[,1]))
max_slope = max(c(shuffled_est_df[,1], est_df[,1]))

# Calculate the combined range for the p-values
min_pvalue = min(c(shuffled_est_df[,2], est_df[,2]))
max_pvalue = max(c(shuffled_est_df[,2], est_df[,2]))

# Define uniform break points for both datasets
breaks_slope = seq(min_slope, max_slope, length.out=31)  # 30 bins, adjust number of breaks as needed
breaks_pvalue = seq(min_pvalue, max_pvalue, length.out=31)  # 30 bins

par(mfrow=c(1,2))
# Plot histograms for slope estimates
hist(shuffled_est_df[,1], breaks=breaks_slope, main="Overlay of Slope Estimates", xlab="Slope Value", col=rgb(1, 0, 0, 0.5), ylim=c(0, max(c(hist(shuffled_est_df[,1], breaks=breaks_slope, plot=FALSE)$counts, hist(est_df[,1], breaks=breaks_slope, plot=FALSE)$counts))))
hist(est_df[,1], breaks=breaks_slope, add=TRUE, col=rgb(0, 0, 1, 0.5))
legend("topleft", legend=c("Null", "Labels shuffled"), fill=c(rgb(1, 0, 0, 0.5), rgb(0, 0, 1, 0.5)),bty = "n", cex=0.75)

# Plot histograms for p-values
hist(shuffled_est_df[,2], breaks=breaks_pvalue, main="Overlay of P-values", xlab="P-value", col=rgb(1, 0, 0, 0.5), ylim=c(0, max(c(hist(shuffled_est_df[,2], breaks=breaks_pvalue, plot=FALSE)$counts, hist(est_df[,2], breaks=breaks_pvalue, plot=FALSE)$counts))))
hist(est_df[,2], breaks=breaks_pvalue, add=TRUE, col=rgb(0, 0, 1, 0.5))

```

This suggests that our results are robust to the label switching problem with average estimates shown below.

```{r, echo=FALSE}
# shuffled randomly (null)
print("mean null")
print(colMeans(shuffled_est_df))

# shuffled rate classes (labels switched randomly)
print("mean labels switched")
print(colMeans(est_df))
```


